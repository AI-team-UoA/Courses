{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<div align=\"center\">\n",
        "\n",
        "<img align=\"center\" src=\"https://www.di.uoa.gr/themes/corporate_lite/logo_en.png\" width=\"500\"/>\n",
        "\n",
        "<hr>\n",
        "\n",
        "\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">\n",
        "<h1><strong>YS19 - Artificial Intelligence II <br>Deep Learning for NLP</strong></h1>\n",
        "</div>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<div align=\"center\">\n",
        "<h2>Homework #3 - RNNs/LSTMs/GRUs (1)</h2>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "vJVebAnYRPMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import of packages and libraries including PyTorch\n",
        "\n"
      ],
      "metadata": {
        "id": "UVM3LqZIXWVc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC4T3bvYQlml",
        "outputId": "e455ff64-f8fc-438f-c15d-88a2cadabeed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on: cpu\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import numpy as np\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import sklearn.metrics\n",
        "import seaborn as sns\n",
        "import random\n",
        "import sys\n",
        "from IPython.display import Image\n",
        "\n",
        "def set_seed(seed = 1234):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "        This is for REPRODUCIBILITY.\n",
        "    '''\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed()\n",
        "\n",
        "device = 'cpu'\n",
        "print('Working on:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OGzq7fgQlmv"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "4h6gtg2kQlmz"
      },
      "source": [
        "# Vanilla RNN\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## **What's an RNN**❓ \n",
        "\n",
        "\n",
        "RNNs model **sequential data**, meaning they have **sequential memory**. \n",
        "An RNN takes in different kind of inputs (text, words, letters, parts of an image, sounds, etc.) in the form of vectors.\n",
        "\n",
        "It returns different kinds of outputs, such as the next word/letter in a sequence or paired with an feed-forward NN it can return a class etc.\n",
        "\n",
        "**Hidden states**\n",
        "\n",
        "From the relationship between hidden layer outputs $h_t$  and $h_{t-1}$ of adjacent time steps, we know that these variables captured and retained the sequence’s historical information up to their current time step, just like the state or memory of the neural network’s current time step. Therefore, such a **hidden layer output is called a hidden state**.\n",
        "\n",
        "> It is noteworthy that __hidden layers__ and __hidden states__ refer to two very different concepts. Hidden layers are, as explained, layers that are hidden from view on the path from input to output. Hidden states are technically speaking inputs to whatever we do at a given step, and they can only be computed by looking at data at previous time steps.\n",
        "\n",
        "\n",
        "**How RNN works**:\n",
        "\n",
        "- It uses previous information to affect later ones\n",
        "- The loop: passes the input forward sequentialy, while *retaining information* about it\n",
        "- This info is stored in the *hidden state*\n",
        "- There are only 3 matrices  that contain weights as parameters. These *DON'T change* with the input, they stay the same through the entire sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "I6nMPjXNQlmy"
      },
      "source": [
        "<img src=\"https://github.com/AI-team-UoA/Courses/blob/main/Deep-Learning-for-NLP-YS19/RNNs/images/rnn_function.png?raw=true\" width=\"1000\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a46NJa3zQlm1"
      },
      "source": [
        "## Language Model in pure pyΤorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "m5v2IaGLQlm2"
      },
      "source": [
        "Neural Nets in general can have different number of inputs/outputs.\n",
        "\n",
        "This is according to the application.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/AI-team-UoA/Courses/blob/main/Deep-Learning-for-NLP-YS19/RNNs/images/input_outputs.jpeg?raw=true\" width=\"1000\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kgIh9ocQlm3"
      },
      "source": [
        "We want to build a simple language model that can learn to predict the next letter in a sequence of letters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "UvUctanrQlm5"
      },
      "source": [
        "<img src=\"https://github.com/AI-team-UoA/Courses/blob/main/Deep-Learning-for-NLP-YS19/RNNs/images/rnn_showcase.png?raw=true\" width=\"1000\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IiPapOeHQlm6"
      },
      "outputs": [],
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "    def __init__(self, num_features, num_hidden, num_classes):\n",
        "\n",
        "        # Syntactic sugar\n",
        "        super().__init__()\n",
        "        \n",
        "        # Needed to formulate the layer shapes\n",
        "        # The size of the embeddings or the # of features\n",
        "        self.num_features = num_features\n",
        "\n",
        "        # The hidden layer mapping size\n",
        "        self.num_hidden = num_hidden\n",
        "        \n",
        "        # The size of the output classes\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # Network Parameters (default requires_grad=True)\n",
        "        # Input\n",
        "        self.Wxh = nn.Parameter(torch.randn((num_features, num_hidden)))\n",
        "        self.Whh = nn.Parameter(torch.randn((num_hidden, num_hidden)))\n",
        "        self.bh  = nn.Parameter(torch.zeros((num_hidden)))\n",
        "        \n",
        "        # Hidden -> Output\n",
        "        self.Why = nn.Parameter(torch.randn((num_hidden, self.num_classes)))\n",
        "        self.by = nn.Parameter(torch.zeros((self.num_classes))) \n",
        "        \n",
        "        # Activation\n",
        "        self.tanh = nn.Tanh()\n",
        "    \n",
        "    def init(self):\n",
        "        # Initialize hidden state to zero\n",
        "        self.h = torch.zeros((self.num_hidden))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.h = self.tanh((x @ self.Wxh) + (self.h @ self.Whh) + self.bh)\n",
        "        y_output = self.h @ self.Why + self.by\n",
        "        return y_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRVR5C3WQlm8"
      },
      "source": [
        "## Questions on pytorch?\n",
        " - Why super() ...?\n",
        " - Why Parameter and no Tensor?\n",
        " > Parameter is a kind of Tensor that is to be considered a module parameter. Parameters are Tensor subclasses, that have a very special property when used with Module s - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn’t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too.\n",
        " - Which activation function for hidden layers? [Check out this for details](https://cs231n.github.io/neural-networks-1/).\n",
        "     - In sort: ReLU (or LeakyRelU) >> tanh >> sigmoid\n",
        "     \n",
        " - @? [PEP-0465](https://legacy.python.org/dev/peps/pep-0465/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsqp6DM9Qlm-",
        "outputId": "60c412d5-95ec-4c25-ee12-a5520c26452f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wxh torch.Size([4, 3])\n",
            "Whh torch.Size([3, 3])\n",
            "bh torch.Size([3])\n",
            "Why torch.Size([3, 4])\n",
            "by torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "num_features, num_hidden, num_classes = 4, 3, 4\n",
        "model = VanillaRNN(num_features, num_hidden, num_classes)\n",
        "\n",
        "# Printing out the module parameters\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph3Ahrq1Qlm_"
      },
      "source": [
        "## Simple Mapper to encode a sequence of chars to vectors\n",
        "\n",
        "\n",
        "### One-Hot encoding\n",
        "\n",
        "When dealing with such categorical data, the most common strategy is to represent each item by a one-hot encoding. A one-hot encoding is a vector whose length is given by the size of the vocabulary N, where all entries are set to 0, except for the entry corresponding to our token, which is set to 1. For example, if the vocabulary had 5 elements, then the one-hot vectors corresponding to indices 0 and 2 would be the following.\n",
        "\n",
        "```\n",
        "[1, 0, 0, 0, 0]\n",
        "[0, 0, 1, 0, 0]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgphdldZQlm_",
        "outputId": "7d83374c-e4d2-44c3-f80c-ce09c67eb1be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:  .,;!-':abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ \n",
            "Size: 60\n"
          ]
        }
      ],
      "source": [
        "import unicodedata\n",
        "import string\n",
        "\n",
        "# Flag to limit the vocabulary to lowercase letters only\n",
        "lowercase_only = False\n",
        "if lowercase_only:\n",
        "    # the lowercase letters (a-z) + some special chars will be in our vocab\n",
        "    characters = \" .,;!-':\" + string.ascii_lowercase\n",
        "else:\n",
        "    # All letters (a-Z) + some special chars will be in our vocab\n",
        "    characters = \" .,;!-':\" + string.ascii_letters\n",
        "    \n",
        "print(f'Vocabulary: {characters} \\nSize: {len(characters)}')\n",
        "\n",
        "class SequenceHandler:\n",
        "    \"\"\"\n",
        "      Utility class to vectorize a sequence of characters. \n",
        "      We will create one instance of this class for every string we want to feed to our model.\n",
        "    \"\"\"\n",
        "    def __init__(self, original_string, max_number_char=-1, lowercase_only=False):\n",
        "        \"\"\"\n",
        "        - original_string: str, the input string to vectorize\n",
        "        - max_number_char: int, whether we want to pad or trim the input sequence. \n",
        "            If max_number_char > 0 the input sequence will be trimmed to max_number_char or will be padded with whitespace\n",
        "        - lowercase_only: bool,\n",
        "            If True, lowercase the input string first to avoid dropping OOV chars.\n",
        "        \"\"\"\n",
        "        \n",
        "        # If lowercase vocab, lowercase the input string as well to avoid dropping the\n",
        "        # OOV capital letters\n",
        "        if lowercase_only:\n",
        "            input_string = original_string.lower()\n",
        "        else:\n",
        "            input_string = original_string\n",
        "\n",
        "        # Keep only letters in the vocab\n",
        "        input_string= \"\".join([s for s in input_string if s in characters])\n",
        "        \n",
        "        # Trim or pad the sequence\n",
        "        if max_number_char > 0 :\n",
        "            if len(input_string) < max_number_char:\n",
        "                input_string = input_string + \" \" * (max_number_char - len(input_string))\n",
        "            else:\n",
        "                input_string = input_string[:max_number_char]\n",
        "            \n",
        "        print(f'Processed string: {original_string} --> {input_string}')\n",
        "        \n",
        "        # Save the input string\n",
        "        self.string = input_string\n",
        "        self.num_characters = len(characters)\n",
        "        \n",
        "        # Mapper to char and back\n",
        "        self.char_to_idx = { ch : i for i, ch in enumerate(characters) }\n",
        "        self.idx_to_char = { i : ch for ch, i in self.char_to_idx.items() }\n",
        "        \n",
        "        self.ohe = self.init_ohe()\n",
        "        print(\"One-hot encoding: \")\n",
        "        print(self.ohe.shape)\n",
        "        print(self.ohe)\n",
        "        self._process()\n",
        "        self._process_for_cl()\n",
        "        \n",
        "    def init_ohe(self):\n",
        "        \"\"\"\n",
        "        This will create a matrix with one hot encoded vectors for each char in the vocab.\n",
        "        Returns:\n",
        "            one_hot (np.ndarray): a collapsed one-hot encoding of all the chars in the vocab.\n",
        "        \"\"\"\n",
        "        one_hot = np.zeros((self.num_characters,self.num_characters), dtype=np.int8)\n",
        "        np.fill_diagonal(one_hot, 1)\n",
        "        return one_hot\n",
        "    \n",
        "    def _process(self):\n",
        "        \"\"\"\n",
        "        This will create the X and y variables of the instance. \n",
        "        In X we will store the vector representation of the sequence in a (N_seq - 1) x embedding_size tensor.\n",
        "        In y we will store the corresponding correct char index value for the next character in the sequence. This will be a tensor of size N_seq, containing integers.\n",
        "        \"\"\"\n",
        "        data_torch = torch.tensor([self.ohe[self.char_to_idx[char]] for char in self.string])\n",
        "        self.X = data_torch[:-1].float()\n",
        "        self.y = torch.argmax(data_torch[1:], dim=1).long()\n",
        "      \n",
        "    # For other purposes\n",
        "    def _process_for_cl(self):\n",
        "        \"\"\"\n",
        "        This will create the X_cl variable of the instance. \n",
        "        In X_cl we will store the vector representation of the sequence in a (N_seq) x embedding_size tensor.\n",
        "        \"\"\"\n",
        "        data_torch = torch.tensor([self.ohe[self.char_to_idx[char]] for char in self.string])\n",
        "        self.X_cl = data_torch[:].float()\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.string\n",
        "        \n",
        "    def make_onehot(self, char):\n",
        "        return torch.Tensor(self.ohe[self.char_to_idx[char]]).float()\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgkBHm0YQlnB",
        "outputId": "decffb4a-2ce0-42fc-de33-2b097576ef10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed string: hello --> hello\n",
            "One-hot encoding: \n",
            "(60, 60)\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 1]]\n",
            "hello (length: 5)\n",
            "X vectors: torch.Size([4, 60]), Y: torch.Size([4])\n",
            "Input: (in chars)\n",
            "['h', 'e', 'l', 'l']\n",
            "Corresponding target output: (in chars)\n",
            "['e', 'l', 'l', 'o']\n"
          ]
        }
      ],
      "source": [
        "data = SequenceHandler('hello')\n",
        "print(f'{data} (length: {len(data.string)})')\n",
        "print(f'X vectors: {data.X.shape}, Y: {data.y.shape}')\n",
        "print(f'Input: (in chars)')\n",
        "print([data.idx_to_char[x_i.item()] for x_i in data.X.argmax(dim=1)])\n",
        "print(f'Corresponding target output: (in chars)')\n",
        "print([data.idx_to_char[y_i.item()] for y_i in data.y])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SV_qQj1VQlnC"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    \"\"\"\n",
        "      Wrapper class that will hold the interface for VanillaRNN, LSTMs and GRUs\n",
        "    \"\"\"\n",
        "    cells = {\n",
        "        \"vanilla\" : VanillaRNN,\n",
        "        # \"LSTM\"    : LSTMCell, # for future implementations\n",
        "        # \"GRU\"     : GRUCell # for future implementations\n",
        "    }\n",
        "    \n",
        "    def __init__(self, \n",
        "                 num_features, \n",
        "                 num_hidden=10, \n",
        "                 num_classes=None, \n",
        "                 cell_type='vanilla'):\n",
        "        super().__init__()\n",
        "\n",
        "        # We default to LM\n",
        "        if num_classes == None:\n",
        "            num_classes = num_features\n",
        "        self.cell_type = cell_type\n",
        "        print(f\"Creating RNN with cell: {cell_type}\")\n",
        "        self.cell = RNN.cells[cell_type](num_features, num_hidden, num_classes)\n",
        "        \n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        for param in self.cell.parameters():\n",
        "\n",
        "            # Keep track of gradient for backprop\n",
        "            param.requires_grad_(True)\n",
        "            \n",
        "            # If we deal with weights xavier initialization\n",
        "            if param.data.ndimension() >= 2:\n",
        "                nn.init.xavier_uniform_(param.data)\n",
        "            \n",
        "            # Else is a bias term so all zeros\n",
        "            else: \n",
        "                nn.init.zeros_(param.data)\n",
        "                \n",
        "    def forward(self, X):\n",
        "\n",
        "        # Setup outputs container (the output at each step)\n",
        "        outputs = torch.zeros_like(X)\n",
        "        \n",
        "        # Iterate through sequence\n",
        "        self.cell.init()\n",
        "        for i, x in enumerate(X):\n",
        "            outputs[i] = self.cell(x)\n",
        "            \n",
        "        return outputs\n",
        "    \n",
        "    def generate(self, \n",
        "                 data, \n",
        "                 init_char, \n",
        "                 num_steps=5, \n",
        "                 output_type='char'):\n",
        "        \"\"\"\n",
        "        Generate text of length num_steps given an initial character.\n",
        "        \"\"\"\n",
        "        # Check for valid character\n",
        "        if init_char not in data.char_to_idx:\n",
        "            avail_chars = \",\".join(data.char_to_idx.keys())\n",
        "            print(f\"Character not in vocab. Pick another from: {avail_chars}\")\n",
        "            return\n",
        "        \n",
        "        # Use both see for example: https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/3\n",
        "        with torch.no_grad():\n",
        "            self.eval()\n",
        "\n",
        "            # Setup feed that will be used to create the next char\n",
        "            feed = torch.zeros((num_steps, data.num_characters))\n",
        "\n",
        "            # First time feed buffer contains only the first string\n",
        "            feed[0] = data.make_onehot(init_char).unsqueeze(dim=0)\n",
        "            \n",
        "            # The output starts obv. with the first given char\n",
        "            output = [init_char]\n",
        "            \n",
        "            # For the rest of wanted chars\n",
        "            for predict_i in range(num_steps-1):\n",
        "                \n",
        "                # Update the feed buffer up to the current char\n",
        "                feed_in = feed[:predict_i+1]\n",
        "                \n",
        "                # Predict the next char in the sequence\n",
        "                next_chars = self(feed_in)[-1]\n",
        "                \n",
        "                # Get the next char id\n",
        "                next_char_idx = torch.argmax(next_chars).item()\n",
        "                \n",
        "                # Get the next char\n",
        "                next_char = data.idx_to_char[next_char_idx]\n",
        "                \n",
        "                # One hot encode the char and update the feed buffer\n",
        "                feed[predict_i+1] = data.make_onehot(next_char).unsqueeze(dim=0)\n",
        "                \n",
        "                # Update the final output\n",
        "                output.append(next_char)\n",
        "\n",
        "            # Concatenate the chars predicted\n",
        "            if output_type == 'char':\n",
        "                output_str = \"\".join(output)\n",
        "            elif output_type == 'word':\n",
        "                output_str = \" \".join(output)\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "\n",
        "            return output_str"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comments:\n",
        "\n",
        "- The __forward method__ defines how to compute the output and hidden state at any time step, given the current input and the state of the model at the previous time step. Note that the RNN model loops through the outermost dimension of inputs, updating the hidden state one time step at a time."
      ],
      "metadata": {
        "id": "Yvd4k5XwIN9h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j03aDUfYQlnD"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1nckK6JQlnE",
        "outputId": "6ce51c62-898f-48a6-8bb0-a1ac56148fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating RNN with cell: vanilla\n",
            "cell.Wxh torch.Size([60, 50])\n",
            "cell.Whh torch.Size([50, 50])\n",
            "cell.bh torch.Size([50])\n",
            "cell.Why torch.Size([50, 60])\n",
            "cell.by torch.Size([60])\n"
          ]
        }
      ],
      "source": [
        "CELL_TYPE = 'vanilla' # vanilla\n",
        "num_hidden = 50\n",
        "net = RNN(num_features=60, \n",
        "          num_hidden=num_hidden, \n",
        "          cell_type=CELL_TYPE)\n",
        "\n",
        "for name, param in net.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.shape)\n",
        "\n",
        "LR = 0.001\n",
        "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "N_EPOCHS = 100\n",
        "print_every_ = 1\n",
        "end_early = False\n",
        "seq_i = \"\"        \n",
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1miR_OmJQlnF",
        "outputId": "7e31c542-4f98-4506-ad7d-d85ed1278dd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpoch 1/100 -- Loss: 4.1822 -- Network out: ht-U-\n",
            "\rEpoch 2/100 -- Loss: 4.1369 -- Network out: ht-U-\n",
            "\rEpoch 3/100 -- Loss: 4.0920 -- Network out: ht-U-\n",
            "\rEpoch 4/100 -- Loss: 4.0475 -- Network out: ht-U-\n",
            "\rEpoch 5/100 -- Loss: 4.0033 -- Network out: ht-U-\n",
            "\rEpoch 6/100 -- Loss: 3.9594 -- Network out: ht-U-\n",
            "\rEpoch 7/100 -- Loss: 3.9157 -- Network out: ht-U-\n",
            "\rEpoch 8/100 -- Loss: 3.8720 -- Network out: ht-U-\n",
            "\rEpoch 9/100 -- Loss: 3.8282 -- Network out: ht-U-\n",
            "\rEpoch 10/100 -- Loss: 3.7843 -- Network out: ht-U-\n",
            "\rEpoch 11/100 -- Loss: 3.7401 -- Network out: hehl-\n",
            "\rEpoch 12/100 -- Loss: 3.6956 -- Network out: hehl-\n",
            "\rEpoch 13/100 -- Loss: 3.6506 -- Network out: hehlo\n",
            "\rEpoch 14/100 -- Loss: 3.6049 -- Network out: hehlo\n",
            "\rEpoch 15/100 -- Loss: 3.5586 -- Network out: hehlo\n",
            "\rEpoch 16/100 -- Loss: 3.5116 -- Network out: hehlo\n",
            "\rEpoch 17/100 -- Loss: 3.4636 -- Network out: hehlo\n",
            "\rEpoch 18/100 -- Loss: 3.4148 -- Network out: heXMo\n",
            "\rEpoch 19/100 -- Loss: 3.3649 -- Network out: heXMo\n",
            "\rEpoch 20/100 -- Loss: 3.3140 -- Network out: hello\n",
            "\n",
            "Ending early. Converged in 19 epochs.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x864 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last Output:  tensor([[ 0.1980, -0.0538, -0.0194,  0.1280, -0.1116, -0.0205, -0.2556, -0.1488,\n",
            "         -0.1919,  0.0942,  0.1398, -0.2608,  0.4029,  0.1110, -0.2841, -0.0838,\n",
            "         -0.0808, -0.0808, -0.1033,  0.2159, -0.3638, -0.0933,  0.2076,  0.0664,\n",
            "         -0.1343,  0.1058, -0.1157,  0.2689, -0.1321, -0.0088, -0.0929,  0.0932,\n",
            "          0.0103, -0.2244, -0.0203, -0.3138, -0.1308,  0.1610,  0.0530, -0.1401,\n",
            "          0.0543, -0.0597,  0.2261, -0.0398, -0.2091, -0.0219, -0.0593,  0.2413,\n",
            "         -0.0228,  0.0789,  0.0790, -0.2553,  0.0477, -0.1935, -0.1930, -0.1498,\n",
            "         -0.1287, -0.1787, -0.1480,  0.1305],\n",
            "        [ 0.2060,  0.1831, -0.3431, -0.0273, -0.0360,  0.1427, -0.0487, -0.1122,\n",
            "         -0.0910,  0.1935, -0.1512, -0.1774,  0.0409, -0.2532,  0.2465,  0.2728,\n",
            "         -0.3141,  0.1985, -0.0588,  0.2525, -0.1591, -0.5165, -0.2155,  0.2668,\n",
            "         -0.0938, -0.0680,  0.0899, -0.1688, -0.0510, -0.0066,  0.0600,  0.0353,\n",
            "         -0.1124, -0.1392,  0.0183,  0.0450,  0.0482, -0.1303, -0.1326,  0.1663,\n",
            "         -0.1297,  0.1766, -0.0574, -0.0838, -0.2657, -0.2761,  0.0904, -0.2767,\n",
            "          0.0936, -0.1984, -0.0175,  0.0072, -0.1968, -0.2340,  0.1487, -0.3910,\n",
            "          0.1545,  0.2737, -0.3386,  0.0282],\n",
            "        [-0.1601, -0.7531, -0.1231, -0.2226, -0.5377, -0.3192, -0.1603, -0.3478,\n",
            "         -0.1909,  0.1171, -0.2627, -0.0426, -0.1541,  0.5242, -0.5705, -0.3429,\n",
            "         -0.2186, -0.0450,  0.1335,  0.7922, -0.0033, -0.0735, -0.4789, -0.5128,\n",
            "          0.3656, -0.3315, -0.2177,  0.2477, -0.2081,  0.0587,  0.0402, -0.3213,\n",
            "         -0.1147,  0.0916, -0.2617, -0.2262,  0.2701,  0.0524, -0.1295, -0.3624,\n",
            "         -0.3508,  0.0374, -0.4189, -0.1327, -0.2030, -0.1386,  0.2595,  0.0922,\n",
            "         -0.5709, -0.0909,  0.0310, -0.3355,  0.0689,  0.1312, -0.0802,  0.0813,\n",
            "         -0.1793, -0.3553,  0.2543, -0.6625],\n",
            "        [-0.3042,  0.2323, -0.5116, -0.3191, -0.7594,  0.4402,  0.1760, -0.7992,\n",
            "          0.5328, -0.2054,  0.1198, -0.3929,  0.6607,  0.3547, -0.4262, -0.1870,\n",
            "         -0.1148,  0.0528,  0.1313,  0.1610, -0.1316, -0.3321,  1.4940, -0.4073,\n",
            "         -0.0402,  0.3203, -0.3090,  0.3249, -0.0702, -0.4340, -0.2634,  0.1718,\n",
            "          0.0339, -0.2505, -0.5952, -0.3098, -0.3079, -0.3005, -0.4609, -0.5383,\n",
            "         -0.0156, -1.1625, -0.5858, -0.5588, -0.1533, -0.9398, -0.7353, -0.1940,\n",
            "         -0.6466,  0.1131, -0.1446, -0.7072, -0.1033,  0.1027, -0.0597, -0.1661,\n",
            "         -0.1231, -0.0056,  0.1826,  0.2113]], grad_fn=<CopySlices>)\n"
          ]
        }
      ],
      "source": [
        " # Ensure net in training mode\n",
        "net.train()\n",
        "\n",
        "for epoch_i in range(N_EPOCHS):\n",
        "\n",
        "    # Zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Get net output, calculate loss, and generate gradients\n",
        "    output = net(data.X)\n",
        "    loss = criterion(output, data.y)\n",
        "\n",
        "    # Generate gradients via autograd\n",
        "    loss.backward() \n",
        "    \n",
        "    # Step\n",
        "    # -----------------------------------\n",
        "    # Clip params\n",
        "    for param in net.parameters():\n",
        "        if param.grad is None:\n",
        "            continue\n",
        "        grad_val = torch.clamp(param.grad, -5, 5)\n",
        "    optimizer.step()\n",
        "    # -----------------------------------\n",
        "    \n",
        "    # Track loss\n",
        "    # CE loss\n",
        "    losses.append(loss.item())\n",
        "    \n",
        "    # Qualitative Eval\n",
        "    if epoch_i % print_every_ == 0:\n",
        "        seq_i = net.generate(data, data.string[0], num_steps=len(data.string))\n",
        "        \n",
        "        if seq_i == data.string:\n",
        "            end_early = True\n",
        "            \n",
        "        # Stdout\n",
        "        # --------------------------------\n",
        "        str_ = f'\\rEpoch {epoch_i+1}/{N_EPOCHS} -- Loss: {losses[-1]:0.4f} -- Network out: {seq_i}'\n",
        "        print(str_)\n",
        "        # --------------------------------\n",
        "    \n",
        "    if end_early:\n",
        "        print(f\"\\nEnding early. Converged in {epoch_i} epochs.\")\n",
        "        break\n",
        "        \n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('# of epochs', fontsize=12)\n",
        "plt.ylabel('CE - Loss', fontsize=12)\n",
        "plt.show()\n",
        "print(\"Last Output: \", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWYND9yfQlnG"
      },
      "source": [
        "### Questions\n",
        "- Do we really need all these epochs? Try tweaking hidden and LR? What's the expected outcome.\n",
        "- What's the final output? How do we get predictions. Can we calculate accuracy?\n",
        "- Qualitative early stopping. In the future what?\n",
        "- Better use a function to wrap-around..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgrWaoafQlnG",
        "outputId": "309c36aa-a66a-48e2-85d1-b874151cb98a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 75.00 %\n"
          ]
        }
      ],
      "source": [
        "def get_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Simple utility function to get the accuracy given the tensors with the true values (y_true) and\n",
        "    a tensor with logits or probas.\n",
        "    Input:\n",
        "    - y_true: torch.Tensor,\n",
        "    shape N, containing the class index for N samples\n",
        "    - y_pred: torch.Tensor,\n",
        "    shape N x num_classes, containing logit / probas values on each cell for the corresponding class per sample\n",
        "    Output: accuracy, float range 0-1\n",
        "    \"\"\"\n",
        "    return (y_pred.argmax(dim=1) == y_true).sum().item() / float(len(y_true))\n",
        "print(f'Accuracy: {100 * get_accuracy(data.y, output):.2f} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzcH8rB-QlnH",
        "outputId": "14d7561a-0018-4993-d24b-fbf90f46d194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class: (12) e : 2.55 %\n",
            "Class: (27) t : 2.23 %\n",
            "Class: (47) N : 2.17 %\n",
            "Class: (42) I : 2.13 %\n",
            "Class: (19) l : 2.11 %\n",
            "Class: (22) o : 2.09 %\n",
            "Class: (0)   : 2.07 %\n",
            "Class: (37) D : 2.00 %\n",
            "Class: (10) c : 1.96 %\n",
            "Class: (59) Z : 1.94 %\n",
            "Class: (3) ; : 1.93 %\n",
            "Class: (13) f : 1.90 %\n",
            "Class: (25) r : 1.89 %\n",
            "Class: (9) b : 1.87 %\n",
            "Class: (31) x : 1.87 %\n",
            "Class: (50) Q : 1.84 %\n",
            "Class: (49) P : 1.84 %\n",
            "Class: (23) p : 1.82 %\n",
            "Class: (40) G : 1.80 %\n",
            "Class: (38) E : 1.79 %\n",
            "Class: (52) S : 1.78 %\n",
            "Class: (32) y : 1.72 %\n",
            "Class: (29) v : 1.69 %\n",
            "Class: (2) , : 1.67 %\n",
            "Class: (34) A : 1.67 %\n",
            "Class: (5) - : 1.67 %\n",
            "Class: (45) L : 1.66 %\n",
            "Class: (48) O : 1.66 %\n",
            "Class: (43) J : 1.63 %\n",
            "Class: (1) . : 1.61 %\n",
            "Class: (46) M : 1.60 %\n",
            "Class: (41) H : 1.60 %\n",
            "Class: (17) j : 1.57 %\n",
            "Class: (16) i : 1.57 %\n",
            "Class: (15) h : 1.56 %\n",
            "Class: (30) w : 1.55 %\n",
            "Class: (21) n : 1.55 %\n",
            "Class: (18) k : 1.53 %\n",
            "Class: (4) ! : 1.52 %\n",
            "Class: (26) s : 1.52 %\n",
            "Class: (56) W : 1.50 %\n",
            "Class: (36) C : 1.49 %\n",
            "Class: (28) u : 1.49 %\n",
            "Class: (24) q : 1.49 %\n",
            "Class: (39) F : 1.48 %\n",
            "Class: (58) Y : 1.47 %\n",
            "Class: (7) : : 1.47 %\n",
            "Class: (55) V : 1.46 %\n",
            "Class: (57) X : 1.42 %\n",
            "Class: (8) a : 1.40 %\n",
            "Class: (54) U : 1.40 %\n",
            "Class: (53) T : 1.40 %\n",
            "Class: (44) K : 1.38 %\n",
            "Class: (33) z : 1.36 %\n",
            "Class: (51) R : 1.32 %\n",
            "Class: (6) ' : 1.32 %\n",
            "Class: (11) d : 1.31 %\n",
            "Class: (14) g : 1.28 %\n",
            "Class: (35) B : 1.24 %\n",
            "Class: (20) m : 1.18 %\n"
          ]
        }
      ],
      "source": [
        "# To get probas from logits we can use the F.softmax function\n",
        "# Get sorted probas per class for first input. First input was 'h' and we expect the most probable value to be 'e'\n",
        "sorted_values, sorted_class_indexes = F.softmax(output[0,:], dim=0).sort(descending=True)\n",
        "\n",
        "for i, class_index in enumerate(sorted_class_indexes.data.numpy()):\n",
        "    print(f'Class: ({class_index}) {data.idx_to_char[class_index]} : {100*sorted_values[i]:.2f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FZgQ3aPQlnI"
      },
      "source": [
        "#### Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6sPSW8RdQlnJ",
        "outputId": "6f877e0d-dfa0-4e15-c7d3-20096a238a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with letter: h\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "init_char = data.string[0]\n",
        "print(f'Starting with letter: {init_char}')\n",
        "net.generate(data, init_char, num_steps=len(data.string[0:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradients explode or vanish 💣 - the curse of RNNs\n",
        "\n",
        "While you are already used to thinking of neural networks as “deep” in the sense that many layers separate the input and output even within a single time step, the length of the sequence introduces a new notion of depth. In addition to the passing through the network in the input-to-output direction, inputs at the first time step must pass through a chain of T layers along the time steps in order to influence the output of the model at the final time step. Taking the backwards view, in each iteration, we backpropagate gradients through time, resulting in a chain of matrix-products with length O(T). This can result in numerical instability, causing the gradients to either explode or vanish depending on the properties of the weight matrices.\n",
        "\n",
        "Dealing with vanishing and exploding gradients is a fundamental problem when designing RNNs and has inspired some of the biggest advances in modern neural network architectures. \n",
        "\n",
        "### Gradient Clipping (heuristic) for the rescue\n",
        "\n",
        "This ensures that the gradient norm never exceeds the radius and that the updated gradient is entirely aligned with the original direction of gradient. It also has the desirable side-effect of limiting the influence any given minibatch (and within it any given sample) can exert on the parameter vector. This bestows a certain degree of robustness to the model. To be clear, it’s a hack. Gradient clipping means that we are not always following the true gradient and it’s hard to reason analytically about the possible side effects. However, it’s a very useful hack, and is widely adopted in RNN implementations in most deep learning frameworks.\n",
        "\n"
      ],
      "metadata": {
        "id": "d6vgVdLUJfhx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeH6WNo7QlnK"
      },
      "source": [
        "# LSTM - Long Short-Term Memory\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Although **gradient clipping** helps with **exploding gradients**, handling **vanishing gradients** appears to require a more elaborate solution. One of the first and most successful techniques for addressing vanishing gradients came in the form of the long short-term memory (LSTM) model due to Hochreiter and Schmidhuber (1997). LSTMs resemble standard recurrent neural networks but here each ordinary recurrent node is replaced by a **memory cell**. Each memory cell contains an internal state, i.e., a node with a self-connected recurrent edge of fixed weight 1, ensuring that the gradient can pass across many time steps without vanishing or exploding.\n",
        "\n",
        "\n",
        "### Memory cell\n",
        "\n",
        "Each memory cell is equipped with an internal state and a number of multiplicative gates that determine whether \n",
        "1. a given input should impact the internal state (the input gate), \n",
        "2. the internal state should be flushed to  (the forget gate), and \n",
        "3. the internal state of a given neuron should be allowed to impact the cell’s output (the output gate).\n",
        "\n",
        "The data feeding into the LSTM gates are the input at the current time step and the hidden state of the previous time step, as illustrated in Fig. 10.1.1. Three fully connected layers with sigmoid activation functions compute the values of the input, forget, and output gates. As a result of the sigmoid activation, all values of the three gates are in the range of (0, 1). Additionally, we require an input node, typically computed with a tanh activation function. \n",
        "\n",
        "Intuitively:\n",
        "- the input gate determines how much of the input node’s value should be added to the current memory cell internal state\n",
        "- the forget gate determines whether to keep the current value of the memory or flush it\n",
        "- the output gate determines whether the memory cell should influence the output at the current time step.\n",
        "\n",
        "<img src=\"https://github.com/AI-team-UoA/Courses/blob/main/Deep-Learning-for-NLP-YS19/RNNs/images/fig1011.png?raw=true\" width=\"700\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0Gt95a2QlnK"
      },
      "source": [
        "<img src=\"https://github.com/AI-team-UoA/Courses/blob/main/Deep-Learning-for-NLP-YS19/RNNs/images/lstm.png?raw=true\" width=\"1000\">\n",
        "\n",
        "\n",
        "Link for more: https://d2l.ai/chapter_recurrent-modern/lstm.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: What's the difference between Vanilla RNNs and LSTMs?\n",
        "\n",
        "> The latter support gating of the hidden state. This means that we have dedicated mechanisms for when a hidden state should be updated and also when it should be reset. These mechanisms are learned and they address the concerns listed above. For instance, if the first token is of great importance we will learn not to update the hidden state after the first observation. Likewise, we will learn to skip irrelevant temporary observations. Last, we will learn to reset the latent state whenever needed. We discuss this in detail below.\n",
        "\n"
      ],
      "metadata": {
        "id": "fjt5RigpOWw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of LSTM from scratch"
      ],
      "metadata": {
        "id": "_Z74qViYgTzb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vbpemQoDQlnK"
      },
      "outputs": [],
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, num_features, num_hidden, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_features = num_features\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # Network Parameters\n",
        "        # New cell content\n",
        "        self.Wxh = nn.Parameter(torch.randn((num_features, num_hidden)))\n",
        "        self.Whh = nn.Parameter(torch.randn((num_hidden, num_hidden)))\n",
        "        self.bh = nn.Parameter(torch.zeros((num_hidden)))\n",
        "        \n",
        "        # Input gate parameters\n",
        "        self.Wxh_i = nn.Parameter(torch.randn_like(self.Wxh))\n",
        "        self.Whh_i = nn.Parameter(torch.randn_like(self.Whh))\n",
        "        self.bh_i = nn.Parameter(torch.randn_like(self.bh))\n",
        "        \n",
        "        # Forget gate parameters\n",
        "        self.Wxh_f = nn.Parameter(torch.randn_like(self.Wxh))\n",
        "        self.Whh_f = nn.Parameter(torch.randn_like(self.Whh))\n",
        "        self.bh_f = nn.Parameter(torch.randn_like(self.bh))\n",
        "        \n",
        "        # Output gate parameters\n",
        "        self.Wxh_o = nn.Parameter(torch.randn_like(self.Wxh))\n",
        "        self.Whh_o = nn.Parameter(torch.randn_like(self.Whh))\n",
        "        self.bh_o = nn.Parameter(torch.randn_like(self.bh))\n",
        "        \n",
        "        # Hidden -> Output\n",
        "        self.Why = nn.Parameter(torch.randn((num_hidden, self.num_classes)))\n",
        "        self.by = nn.Parameter(torch.zeros((self.num_classes))) \n",
        "        \n",
        "        # Activations\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def init(self):\n",
        "        self.h = torch.zeros((self.num_hidden))  # Hidden state\n",
        "        self.c = torch.zeros((self.num_hidden))  # Cell state\n",
        "        \n",
        "    def forward(self, x):\n",
        "        potential_input = self.tanh((x @ self.Wxh) + (self.h @ self.Whh + self.bh))\n",
        "        \n",
        "        # Gate updates\n",
        "        input_gate = self.sigmoid((x @ self.Wxh_i) + (self.h @ self.Whh_i + self.bh_i))\n",
        "        forget_gate = self.sigmoid((x @ self.Wxh_f) + (self.h @ self.Whh_f + self.bh_f))\n",
        "        output_gate = self.sigmoid((x @ self.Wxh_o) + (self.h @ self.Whh_o + self.bh_o))\n",
        "        \n",
        "        # Update c and h\n",
        "        self.c = self.c * forget_gate + potential_input * input_gate\n",
        "        self.h = output_gate * self.tanh(self.c)\n",
        "        \n",
        "        y_output = self.h @ self.Why + self.by\n",
        "        \n",
        "        return y_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX6jFsEDQlnL"
      },
      "source": [
        "## Update setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "mPBB0dO4QlnL",
        "outputId": "c1123676-dc12-4c20-ed20-31b1c50e614f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpoch 1/500 -- Loss: 4.1151 -- Network out: hDwJc\n",
            "Epoch 11/500 -- Loss: 4.0250 -- Network out: hNbgm\n",
            "Epoch 21/500 -- Loss: 3.9296 -- Network out: hloel\n",
            "Epoch 31/500 -- Loss: 3.8114 -- Network out: hloel\n",
            "Epoch 41/500 -- Loss: 3.6418 -- Network out: hello\n",
            "\n",
            "Ending early. Converged in 40 epochs.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "RNN.cells['LSTM'] = LSTMCell\n",
        "CELL_TYPE = 'LSTM'\n",
        "num_hidden = 50\n",
        "net = RNN(num_features=data.num_characters, \n",
        "          num_hidden=num_hidden, \n",
        "          cell_type=CELL_TYPE)\n",
        "\n",
        "LR = 0.001\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "losses = []\n",
        "\n",
        "N_EPOCHS = 500\n",
        "print_every_ = 10\n",
        "end_early = False\n",
        "seq_i = \"\"\n",
        "\n",
        "\n",
        " # Ensure net in training mode\n",
        "net.train()\n",
        "\n",
        "for epoch_i in range(N_EPOCHS):\n",
        "\n",
        "    # Zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Get net output, calculate loss, and generate gradients\n",
        "    output = net(data.X)\n",
        "    loss = criterion(output, data.y)\n",
        "\n",
        "    # Generate gradients via autodiff\n",
        "    loss.backward() \n",
        "    \n",
        "    # Step\n",
        "    # -----------------------------------\n",
        "    # Clip params\n",
        "    for param in net.parameters():\n",
        "        if param.grad is None:\n",
        "            continue\n",
        "        grad_val = torch.clamp(param.grad, -5, 5)\n",
        "    optimizer.step()\n",
        "    # -----------------------------------\n",
        "    \n",
        "    # Track loss\n",
        "    # CE loss\n",
        "    losses.append(loss.item())\n",
        "    \n",
        "    # Qualitative Eval\n",
        "    if epoch_i % print_every_ == 0:\n",
        "        seq_i = net.generate(data, data.string[0], num_steps=len(data.string))\n",
        "        \n",
        "        if seq_i == data.string:\n",
        "            end_early = True\n",
        "            \n",
        "        # Stdout\n",
        "        # --------------------------------\n",
        "        str_ = f'\\rEpoch {epoch_i+1}/{N_EPOCHS} -- Loss: {losses[-1]:0.4f} -- Network out: {seq_i}'\n",
        "        print(str_)\n",
        "        # --------------------------------\n",
        "    \n",
        "    if end_early:\n",
        "        print(f\"\\nEnding early. Converged in {epoch_i} epochs.\")\n",
        "        break\n",
        "    \n",
        "plt.plot(losses)\n",
        "plt.xlabel('# of epochs', fontsize=12)\n",
        "plt.ylabel('CE - Loss', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K5BydAwQlnN"
      },
      "source": [
        "# GRU - Gated Recurrent Units\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The gated recurrent unit (GRU) (Cho et al., 2014) offered a streamlined version of the LSTM memory cell that often achieves comparable performance but with the advantage of being faster to compute.\n",
        "\n",
        "\n",
        "LSTM’s three gates are replaced by two: \n",
        "- the reset gate and \n",
        "- the update gate. \n",
        "\n",
        "As with LSTMs, these gates are given sigmoid activations, forcing their values to lie in the interval (0,1). \n",
        "\n",
        "\n",
        "Intuitively, \n",
        "- the reset gate controls how much of the previous state we might still want to remember\n",
        "- an update gate would allow us to control how much of the new state is just a copy of the old state. \n",
        "\n",
        "GRUs have the following two distinguishing features:\n",
        "\n",
        "- Reset gates help capture short-term dependencies in sequences.\n",
        "- Update gates help capture long-term dependencies in sequences.\n",
        "\n",
        "<img src=\"https://github.com/AI-team-UoA/Courses/blob/main/Deep-Learning-for-NLP-YS19/RNNs/images/gru.png?raw=true\" width=\"1000\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "A9i0VYZoQlnN"
      },
      "outputs": [],
      "source": [
        "class GRUCell(nn.Module):\n",
        "    def __init__(self, num_features, num_hidden, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_features = num_features\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # Network Parameters\n",
        "        # Potential Input\n",
        "        self.Wxh = nn.Parameter(torch.randn((num_features, num_hidden)))\n",
        "        self.Whh = nn.Parameter(torch.randn((num_hidden, num_hidden)))\n",
        "        self.bh = nn.Parameter(torch.zeros((num_hidden)))\n",
        "        \n",
        "        # Update gate parameters\n",
        "        self.Wxh_u = nn.Parameter(torch.randn_like(self.Wxh))\n",
        "        self.Whh_u = nn.Parameter(torch.randn_like(self.Whh))\n",
        "        self.bh_u = nn.Parameter(torch.randn_like(self.bh))\n",
        "        \n",
        "        # Reset gate parameters\n",
        "        self.Wxh_r = nn.Parameter(torch.randn_like(self.Wxh))\n",
        "        self.Whh_r = nn.Parameter(torch.randn_like(self.Whh))\n",
        "        self.bh_r = nn.Parameter(torch.randn_like(self.bh))\n",
        "        \n",
        "        # Hidden -> Output\n",
        "        self.Why = nn.Parameter(torch.randn((num_hidden, self.num_classes)))\n",
        "        self.by = nn.Parameter(torch.zeros((self.num_classes))) \n",
        "        \n",
        "        # Activations\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def init(self):\n",
        "        self.h = torch.zeros((self.num_hidden))  # Hidden state\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Gate updates\n",
        "        update_gate = self.sigmoid((x @ self.Wxh_u) + (self.h @ self.Whh_u + self.bh_u))\n",
        "        reset_gate = self.sigmoid((x @ self.Wxh_r) + (self.h @ self.Whh_r + self.bh_r))\n",
        "        \n",
        "        potential_input = self.tanh((x @ self.Wxh) + (reset_gate @ self.Whh + self.bh))\n",
        "        \n",
        "        self.h = self.h * (1-update_gate) + (potential_input * update_gate)\n",
        "        y_output = self.h @ self.Why + self.by\n",
        "        \n",
        "        return y_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the GRU training"
      ],
      "metadata": {
        "id": "8XBgg4Coi1W1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "coHpWt4AQlnO",
        "outputId": "b3c26810-7c9f-4f29-b86d-4a86fbd13562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpoch 1/500 -- Loss: 4.2630 -- Network out: hMGMG\n",
            "\rEpoch 11/500 -- Loss: 3.3905 -- Network out: hllll\n",
            "Epoch 21/500 -- Loss: 2.6097 -- Network out: hllll\n",
            "Epoch 31/500 -- Loss: 1.9793 -- Network out: hllll\n",
            "Epoch 41/500 -- Loss: 1.5481 -- Network out: hllll\n",
            "Epoch 51/500 -- Loss: 1.2567 -- Network out: hllll\n",
            "Epoch 61/500 -- Loss: 1.0792 -- Network out: hllll\n",
            "Epoch 71/500 -- Loss: 0.9534 -- Network out: hllll\n",
            "Epoch 81/500 -- Loss: 0.8562 -- Network out: hllll\n",
            "Epoch 91/500 -- Loss: 0.7737 -- Network out: helll\n",
            "Epoch 101/500 -- Loss: 0.7003 -- Network out: hello\n",
            "\n",
            "Ending early. Converged in 100 epochs.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "RNN.cells['GRU'] = GRUCell\n",
        "CELL_TYPE = 'GRU'\n",
        "num_hidden = 50\n",
        "net = RNN(num_features=data.num_characters, num_hidden=num_hidden, cell_type=CELL_TYPE)\n",
        "\n",
        "LR = 0.001\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "losses = []\n",
        "\n",
        "N_EPOCHS = 500\n",
        "print_every_ = 10\n",
        "end_early = False\n",
        "seq_i = \"\"\n",
        "\n",
        "\n",
        " # Ensure net in training mode\n",
        "net.train()\n",
        "for epoch_i in range(N_EPOCHS):\n",
        "    # Zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Get net output, calculate loss, and generate gradients\n",
        "    output = net(data.X)\n",
        "    loss = criterion(output, data.y)\n",
        "    # Generate gradients via autodiff\n",
        "    loss.backward() \n",
        "    \n",
        "    # Step\n",
        "    # -----------------------------------\n",
        "    # Clip params\n",
        "    for param in net.parameters():\n",
        "        if param.grad is None:\n",
        "            continue\n",
        "        grad_val = torch.clamp(param.grad, -5, 5)\n",
        "    optimizer.step()\n",
        "    # -----------------------------------\n",
        "    \n",
        "    # Track loss\n",
        "    # CE loss\n",
        "    losses.append(loss.item())\n",
        "    \n",
        "    # Qualitative Eval\n",
        "    if epoch_i % print_every_ == 0:\n",
        "        seq_i = net.generate(data, data.string[0], num_steps=len(data.string))\n",
        "        \n",
        "        if seq_i == data.string:\n",
        "            end_early = True\n",
        "            \n",
        "        # Stdout\n",
        "        # --------------------------------\n",
        "        str_ = f'\\rEpoch {epoch_i+1}/{N_EPOCHS} -- Loss: {losses[-1]:0.4f} -- Network out: {seq_i}'\n",
        "        print(str_)\n",
        "        # --------------------------------\n",
        "    \n",
        "    if end_early:\n",
        "        print(f\"\\nEnding early. Converged in {epoch_i} epochs.\")\n",
        "        break\n",
        "    \n",
        "plt.plot(losses)\n",
        "plt.xlabel('# of epochs', fontsize=12)\n",
        "plt.ylabel('CE - Loss', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyUR-v18QlnO"
      },
      "source": [
        "## Long-term dependecies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te7SVhibQlnP",
        "outputId": "6fa422ad-a3be-4829-f993-25aaeedfac26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed string: The quick brown fox jumps over the lazy dog --> The quick brown fox jumps over the lazy dog\n"
          ]
        }
      ],
      "source": [
        "data = SequenceHandler('The quick brown fox jumps over the lazy dog')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45JUqda6QlnP",
        "outputId": "6145d1e5-f48d-4e8b-a199-158515897190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting vanilla...\n",
            "\n",
            "Ending early. Converged in 20 epochs.\n",
            "Epoch 21/500 -- Loss: 0.4322 -- Network out: The quick brown fox jumps over the lazy dog\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Fitting LSTM...\n",
            "\n",
            "Ending early. Converged in 50 epochs.\n",
            "Epoch 51/500 -- Loss: 0.2151 -- Network out: The quick brown fox jumps over the lazy dog\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Fitting GRU...\n",
            "\n",
            "Ending early. Converged in 40 epochs.\n",
            "Epoch 41/500 -- Loss: 0.4050 -- Network out: The quick brown fox jumps over the lazy dog\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 500\n",
        "LR = 0.01\n",
        "num_hidden = 50\n",
        "\n",
        "for CELL_TYPE in ['vanilla', 'LSTM', 'GRU']:\n",
        "    print(f'Fitting {CELL_TYPE}...')\n",
        "    net = RNN(num_features=data.num_characters, num_hidden=num_hidden, cell_type=CELL_TYPE)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LR)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    losses = []\n",
        "\n",
        "\n",
        "    end_early = False\n",
        "    seq_i = \"\"\n",
        "\n",
        "\n",
        "    net.train() # Ensure net in training mode\n",
        "    for epoch_i in range(N_EPOCHS):\n",
        "        # Zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get net output, calculate loss, and generate gradients\n",
        "        output = net(data.X)\n",
        "        loss = criterion(output, data.y)\n",
        "        loss.backward() # Generate gradients via autodiff\n",
        "\n",
        "        # Step\n",
        "        # -----------------------------------\n",
        "        # Clip params\n",
        "        for param in net.parameters():\n",
        "            if param.grad is None:\n",
        "                continue\n",
        "            grad_val = torch.clamp(param.grad, -5, 5)\n",
        "        optimizer.step()\n",
        "        # -----------------------------------\n",
        "\n",
        "        # Track loss\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Qualitative Eval\n",
        "        if epoch_i % 10 == 0:\n",
        "            seq_i = net.generate(data, data.string[0], num_steps=len(data.string))\n",
        "\n",
        "            if seq_i == data.string:\n",
        "                end_early = True\n",
        "\n",
        "        # Stdout\n",
        "        # --------------------------------\n",
        "        str_ = f'\\rEpoch {epoch_i+1}/{N_EPOCHS} -- Loss: {losses[-1]:0.4f} -- Network out: {seq_i}'\n",
        "        # print(str_)\n",
        "        # --------------------------------\n",
        "\n",
        "        if end_early:\n",
        "            print(f\"\\nEnding early. Converged in {epoch_i} epochs.\")\n",
        "            break\n",
        "    str_ = f'\\rEpoch {epoch_i+1}/{N_EPOCHS} -- Loss: {losses[-1]:0.4f} -- Network out: {seq_i}'\n",
        "    print(str_)\n",
        "    print('~'*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU7a6gPnQlnQ"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Why does this happen?\n",
        "1. Try changing the learning rate (e.g. 0.08).\n",
        "2. Did the results change? If yes why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QoZT9CcQlnR"
      },
      "source": [
        "# Complexity comparison of Vanilla RNNs, LSTMs & GRUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYMJsyTBQlnR",
        "outputId": "49f9b71c-3a87-46b7-ebd3-da04931aa3ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Params:..\n",
            "vanilla : 310560\n",
            "LSTM : 1152060\n",
            "GRU : 871560\n"
          ]
        }
      ],
      "source": [
        "num_features, num_hidden, num_classes = 300, 500, 1\n",
        "print('Number of Params:..')\n",
        "for cell_type in ['vanilla', 'LSTM', 'GRU']:\n",
        "    net = RNN(num_features=data.num_characters, num_hidden=num_hidden, cell_type=cell_type)\n",
        "    net_param_count = 0\n",
        "    for name, param in net.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            c = 1\n",
        "            for val in param.shape:\n",
        "                c *= val\n",
        "        net_param_count += c\n",
        "    print(f'{cell_type} : {net_param_count}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htKe_aygQlnV"
      },
      "source": [
        "# Recap\n",
        "\n",
        "- LSTM / GRU >> RNN. LSTM more effective, GRU more efficient.\n",
        "- Use ReLUs and Adam as go to methods.\n",
        "- Tweak hidden sizes and learning rates according to model performance during training.\n",
        "- Better to create functions: def train(), def evaluate() ...  and also utilize already existing functionalities (Dataset and DataLoader from torch.utils) to avoid messy code and incosistencies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Other resources\n",
        "\n",
        "\n",
        "- Book with accompanying torch code:\n",
        "  - [Deep dive 2 deep learning: RRN Chapter](https://d2l.ai/chapter_recurrent-neural-networks/index.html)\n",
        "- Posts:\n",
        "    - [Karpathys famous blog post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) \n",
        "    - [Colah's blog post on LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "    - [Explained extensive RNN post](https://explained.ai/rnn/index.html)\n",
        "- Applications\n",
        "    - [Surname classification with RNNs](https://github.com/joosthub/PyTorchNLPBook/blob/master/chapters/chapter_6/classifying-surnames/Chapter-6-Surname-Classification-with-RNNs.ipynb)\n",
        "    - [LSTM with trainable word embeddings](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)\n",
        "    - [Emojifying sentences with Glove Embeddings](https://github.com/hiepnguyen034/Emojifier-plus)"
      ],
      "metadata": {
        "id": "NpvHiOHOk94v"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}